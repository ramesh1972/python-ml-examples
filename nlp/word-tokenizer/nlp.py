from nltk.tokenize import  word_tokenize

data = "Hello, Awesome User"

# tokenization of sentence into words
tokens = word_tokenize(data)

# printing the tokens
print(word_tokenize(data))